"""LightRAG ê¸°ë°˜ RAG ì„œë¹„ìŠ¤"""
import asyncio
import time
from typing import List, Dict, Optional, Any
from pathlib import Path
from loguru import logger
from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc
from src.Config.config import settings
from src.Service.llm_service import get_llm_service, get_embedding_llm_service, get_kg_llm_service, LLMService
from src.Service.document_loader import DocumentLoader
from src.Service.local_api_service import LocalApiService

# ì „ì—­ LLM ì„œë¹„ìŠ¤ (ì§ë ¬í™” ë¬¸ì œ í•´ê²°ì„ ìœ„í•´)
_global_llm_service: Optional[LLMService] = None
_global_embedding_service: Optional[LLMService] = None
_global_kg_service: Optional[LLMService] = None
# ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (ìµœëŒ€ 2ê°œì˜ ë™ì‹œ ìš”ì²­ë§Œ í—ˆìš©)
_llm_semaphore = asyncio.Semaphore(2)
_embedding_semaphore = asyncio.Semaphore(3)

# ì„ë² ë”© ì§„í–‰ë¥  ì¶”ì ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤
_embedding_progress = {
    "total_calls": 0,
    "completed_calls": 0,
    "total_texts": 0,
    "completed_texts": 0,
    "start_time": None,
    "document_times": [],  # ê° ë¬¸ì„œë³„ ì²˜ë¦¬ ì‹œê°„
    "current_document": None,
    "document_start_time": None
}


def _reset_embedding_progress():
    """ì„ë² ë”© ì§„í–‰ë¥  ì¶”ì  ë³€ìˆ˜ ì´ˆê¸°í™”"""
    global _embedding_progress
    _embedding_progress = {
        "total_calls": 0,
        "completed_calls": 0,
        "total_texts": 0,
        "completed_texts": 0,
        "start_time": None,
        "document_times": [],
        "current_document": None,
        "document_start_time": None
    }


def _start_document_timing(document_name: str):
    """ë¬¸ì„œë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ì‹œì‘"""
    global _embedding_progress
    _embedding_progress["current_document"] = document_name
    _embedding_progress["document_start_time"] = time.time()
    logger.info(f"ğŸ“„ ë¬¸ì„œ ì„ë² ë”© ì‹œì‘: {document_name}")


def _end_document_timing():
    """ë¬¸ì„œë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ì™„ë£Œ"""
    global _embedding_progress
    if _embedding_progress["document_start_time"] and _embedding_progress["current_document"]:
        elapsed = time.time() - _embedding_progress["document_start_time"]
        _embedding_progress["document_times"].append(elapsed)
        avg_time = sum(_embedding_progress["document_times"]) / len(_embedding_progress["document_times"])
        
        logger.info(f"âœ… ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ: {_embedding_progress['current_document']} "
                   f"(ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ, í‰ê· : {avg_time:.1f}ì´ˆ)")
        
        _embedding_progress["current_document"] = None
        _embedding_progress["document_start_time"] = None


def _log_embedding_progress():
    """í˜„ì¬ ì„ë² ë”© ì§„í–‰ë¥  ë¡œê·¸ ì¶œë ¥"""
    global _embedding_progress
    
    if _embedding_progress["total_texts"] == 0:
        return
    
    completed = _embedding_progress["completed_texts"]
    total = _embedding_progress["total_texts"]
    percentage = (completed / total) * 100
    
    # ì§„í–‰ë¥  ê³„ì‚°
    progress_bar = "â–ˆ" * int(percentage / 5) + "â–‘" * (20 - int(percentage / 5))
    
    # ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡
    if _embedding_progress["start_time"] and completed > 0:
        elapsed = time.time() - _embedding_progress["start_time"]
        rate = completed / elapsed  # í…ìŠ¤íŠ¸/ì´ˆ
        remaining = (total - completed) / rate if rate > 0 else 0
        eta_str = f", ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining:.0f}ì´ˆ" if remaining > 0 else ""
    else:
        eta_str = ""
    
    logger.info(f"ğŸ”„ ì„ë² ë”© ì§„í–‰ë¥ : {completed}/{total} ({percentage:.1f}%) "
               f"[{progress_bar}]{eta_str}")


class RAGService:
    """LightRAGë¥¼ í™œìš©í•œ RAG ì„œë¹„ìŠ¤"""

    # __init__ì€ ë¹„ë™ê¸°ì¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë¹„ë™ê¸° ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ë³„ë„ ë©”ì„œë“œë¥¼ ë§Œë“­ë‹ˆë‹¤.
    def __init__(self, llm_service: LLMService, rag_instance: Optional[LightRAG]):
        self.document_loader = DocumentLoader()
        self.llm_service = llm_service
        self.rag = rag_instance
        self.local_api = LocalApiService()
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        self.conversation_history: list[dict[str, str]] = []  # [{role:"user"|"assistant", content:str}]

    @classmethod
    async def create(cls) -> "RAGService":
        """RAGServiceì˜ ë¹„ë™ê¸° ìƒì„±ì"""
        # ëŒ€í™”ìš© LLM ì„œë¹„ìŠ¤ ìƒì„±
        llm_service = await get_llm_service()
        # embeddingìš© LLM ì„œë¹„ìŠ¤ ìƒì„± (localì¼ ë•ŒëŠ” ollama ì‚¬ìš©)
        embedding_service = await get_embedding_llm_service()
        # Knowledge Graphìš© LLM ì„œë¹„ìŠ¤ ìƒì„± (localì¼ ë•ŒëŠ” ollama ì‚¬ìš©)
        kg_service = await get_kg_llm_service()
        
        rag_instance = await cls.a_initialize_rag(llm_service, embedding_service, kg_service)
        return cls(llm_service, rag_instance)

    @staticmethod
    async def _check_llm_health(llm_service: LLMService, max_attempts: int = 3) -> bool:
        """LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ - ì„¸ì…˜ ìƒíƒœ ì˜¤ë¥˜ ê³ ë ¤"""
        from src.Service.llm_service import LocalLLMService
        
        # ë¡œì»¬ LLM ì„œë¹„ìŠ¤ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if isinstance(llm_service, LocalLLMService):
            logger.info("ë¡œì»¬ LLM API ì„œë¹„ìŠ¤ ê°ì§€ë¨, ì—°ê²° í…ŒìŠ¤íŠ¸ ë°©ì‹ ë³€ê²½")
            # ì´ë¯¸ create()ì—ì„œ ì—°ê²° í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í–ˆìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            return True
        
        for attempt in range(max_attempts):
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
                async with _llm_semaphore:  # ì„¸ë§ˆí¬ì–´ ì‚¬ìš©
                    test_response = await llm_service.generate(
                        "test", 
                        temperature=0.1, 
                        max_tokens=5
                    )
                if test_response and test_response.strip():
                    logger.info(f"LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì™„ë£Œ (ì‹œë„ {attempt + 1}/{max_attempts})")
                    return True
                else:
                    logger.warning(f"LLM ì„œë¹„ìŠ¤ ë¹ˆ ì‘ë‹µ (ì‹œë„ {attempt + 1}/{max_attempts})")
            except Exception as e:
                error_message = str(e).lower()
                
                # ì„¸ì…˜ ìƒíƒœ ì˜¤ë¥˜ëŠ” ì‹¤ì œë¡œëŠ” ì—°ê²° ì„±ê³µì„ ì˜ë¯¸
                if "waiting for user input" in error_message or "session" in error_message:
                    logger.info(f"ì„¸ì…˜ ìƒíƒœ ì˜¤ë¥˜ ê°ì§€ - ì—°ê²°ì€ ì„±ê³µ (ì‹œë„ {attempt + 1}/{max_attempts})")
                    return True
                
                logger.warning(f"LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_attempts}): {e}")
                if attempt < max_attempts - 1:
                    wait_time = 2 ** attempt  # ì§€ìˆ˜ì  ë°±ì˜¤í”„ (2, 4, 8ì´ˆ)
                    logger.info(f"LLM ì„œë¹„ìŠ¤ ì•ˆì •í™” ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
                    await asyncio.sleep(wait_time)
        
        logger.error("LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
        return False

    @staticmethod
    async def a_initialize_rag(llm_service: LLMService, embedding_service: LLMService, kg_service: LLMService) -> Optional[LightRAG]:
        """LightRAG ì´ˆê¸°í™” (ë¹„ë™ê¸°) - ëŒ€í™”ìš©, embeddingìš©, KGìš© ì„œë¹„ìŠ¤ ë¶„ë¦¬"""
        try:
            settings.create_directories()

            # ëŒ€í™”ìš© LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
            logger.info("ëŒ€í™”ìš© LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...")
            if not await RAGService._check_llm_health(llm_service):
                logger.warning("ëŒ€í™”ìš© LLM ì„œë¹„ìŠ¤ê°€ ë¶ˆì•ˆì •í•˜ì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")

            # embeddingìš© LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
            logger.info("Embeddingìš© LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...")
            if not await RAGService._check_llm_health(embedding_service):
                logger.warning("Embeddingìš© LLM ì„œë¹„ìŠ¤ê°€ ë¶ˆì•ˆì •í•˜ì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")

            # Knowledge Graphìš© LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
            logger.info("Knowledge Graphìš© LLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...")
            if not await RAGService._check_llm_health(kg_service):
                logger.warning("Knowledge Graphìš© LLM ì„œë¹„ìŠ¤ê°€ ë¶ˆì•ˆì •í•˜ì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")

            # ì „ì—­ ë³€ìˆ˜ë¡œ LLM ì„œë¹„ìŠ¤ë“¤ ì €ì¥ (ì§ë ¬í™” ë¬¸ì œ í•´ê²°)
            global _global_llm_service, _global_embedding_service, _global_kg_service
            _global_llm_service = llm_service
            _global_embedding_service = embedding_service
            _global_kg_service = kg_service

            # ì„œë¹„ìŠ¤ ì •ë³´ ë¡œê·¸
            chat_provider = settings.llm_provider
            if chat_provider == 'auto':
                chat_provider = settings.get_llm_service()
            
            embedding_provider = settings.get_embedding_llm_service()
            kg_provider = settings.get_kg_llm_service()
            
            logger.info(f"ì„œë¹„ìŠ¤ êµ¬ì„± - ëŒ€í™”: {chat_provider}, Embedding: {embedding_provider}, KG: {kg_provider}")

            # ì§ë ¬í™” ê°€ëŠ¥í•œ ë˜í¼ í•¨ìˆ˜ë“¤ ìƒì„±
            async def llm_model_func(prompt: str, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs) -> str:
                # LightRAGì—ì„œ ì „ë‹¬í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë“¤ì„ ì²˜ë¦¬í•˜ê³  LLM ì„œë¹„ìŠ¤ê°€ ì§€ì›í•˜ëŠ” ê²ƒë§Œ ì „ë‹¬
                filtered_kwargs = {
                    k: v for k, v in kwargs.items() 
                    if k in ['temperature', 'max_tokens']
                }
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆëŠ” ê²½ìš° í¬í•¨
                final_prompt = prompt
                if system_prompt:
                    final_prompt = f"{system_prompt}\n\n{prompt}"
                
                # Knowledge Graph êµ¬ì¶•ê³¼ ì¼ë°˜ ëŒ€í™”ë¥¼ êµ¬ë¶„í•˜ì—¬ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ ì‚¬ìš©
                if keyword_extraction or "extract" in prompt.lower() or "entity" in prompt.lower() or "relationship" in prompt.lower():
                    # Knowledge Graph êµ¬ì¶•ìš© ì„œë¹„ìŠ¤ ì‚¬ìš©
                    target_service = _global_kg_service
                    service_name = "Knowledge Graph"
                else:
                    # ì¼ë°˜ ëŒ€í™”ìš© ì„œë¹„ìŠ¤ ì‚¬ìš©
                    target_service = _global_llm_service
                    service_name = "ëŒ€í™”"
                
                # ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•œ ë™ì‹œì„± ì œì–´
                async with _llm_semaphore:
                    # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ, ê°œì„ ëœ ë°±ì˜¤í”„)
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            if target_service is None:
                                logger.error(f"ì „ì—­ {service_name}ìš© LLM ì„œë¹„ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                                return "LLM ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                            
                            result = await target_service.generate(final_prompt, **filtered_kwargs)
                            if result and result.strip():  # ë¹„ì–´ìˆì§€ ì•Šì€ ì‘ë‹µë§Œ ë°˜í™˜
                                logger.debug(f"{service_name}ìš© LLM ì‘ë‹µ ìƒì„± ì„±ê³µ (ê¸¸ì´: {len(result)})")
                                return result
                            else:
                                logger.warning(f"{service_name}ìš© LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤ (ì‹œë„ {attempt + 1}/{max_retries})")
                        except Exception as e:
                            logger.error(f"{service_name}ìš© LLM ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                            if attempt == max_retries - 1:  # ë§ˆì§€ë§‰ ì‹œë„
                                logger.error("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
                                return f"LLM ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                            
                            # ì¬ì‹œë„ ì „ ëŒ€ê¸° ì‹œê°„ ì¦ê°€ (ì§€ìˆ˜ì  ë°±ì˜¤í”„)
                            wait_time = (2 ** attempt) + 2  # 4, 6, 10ì´ˆë¡œ ì¦ê°€
                            logger.info(f"{service_name}ìš© LLM ì¬ì‹œë„ ëŒ€ê¸° ì¤‘... ({wait_time}ì´ˆ)")
                            await asyncio.sleep(wait_time)
                    
                    return "LLM ì„œë¹„ìŠ¤ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            async def embedding_func(texts):
                """LightRAG í˜¸í™˜ ì„ë² ë”© í•¨ìˆ˜ (embedding ì „ìš© ì„œë¹„ìŠ¤ ì‚¬ìš©, ë™ì‹œì„± ì œì–´ í¬í•¨, ì§„í–‰ë¥  ì¶”ì )"""
                global _embedding_progress
                
                async with _embedding_semaphore:  # ì„ë² ë”© ì„¸ë§ˆí¬ì–´ ì‚¬ìš©
                    try:
                        if _global_embedding_service is None:
                            logger.error("ì „ì—­ embeddingìš© LLM ì„œë¹„ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                            dummy_dim = 1024  # ê¸°ë³¸ ì°¨ì›
                            if isinstance(texts, list) and len(texts) > 1:
                                return [[0.0] * dummy_dim for _ in texts]
                            else:
                                return [0.0] * dummy_dim
                        
                        # ì§„í–‰ë¥  ì¶”ì  ì‹œì‘
                        _embedding_progress["total_calls"] += 1
                        
                        # í…ìŠ¤íŠ¸ ìˆ˜ ê³„ì‚°
                        if isinstance(texts, list):
                            text_count = len(texts)
                        else:
                            text_count = 1
                        
                        _embedding_progress["total_texts"] += text_count
                        
                        logger.debug(f"ì„ë² ë”© ìš”ì²­ (embedding ì „ìš© ì„œë¹„ìŠ¤): {type(texts)}, í…ìŠ¤íŠ¸ ìˆ˜: {text_count}")
                        
                        # ì…ë ¥ ì²˜ë¦¬ - LightRAGëŠ” ë‹¤ì–‘í•œ í˜•íƒœë¡œ í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŒ
                        if isinstance(texts, str):
                            # ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°
                            if not texts.strip():
                                logger.warning("ë¹ˆ ë¬¸ìì—´ì— ëŒ€í•œ ì„ë² ë”©")
                                _embedding_progress["completed_texts"] += 1
                                _log_embedding_progress()
                                return [0.0] * _global_embedding_service.embedding_dim
                            
                            # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                            max_retries = 2
                            for attempt in range(max_retries):
                                try:
                                    result = await _global_embedding_service.embed(texts)
                                    logger.debug(f"ì„ë² ë”© ê²°ê³¼ ì°¨ì›: {len(result)}")
                                    
                                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                                    _embedding_progress["completed_texts"] += 1
                                    _embedding_progress["completed_calls"] += 1
                                    _log_embedding_progress()
                                    
                                    return result  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜
                                except Exception as e:
                                    logger.warning(f"embedding ì„œë¹„ìŠ¤ ì„ë² ë”© ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                                    if attempt < max_retries - 1:
                                        await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
                                    else:
                                        raise
                                
                        elif isinstance(texts, list):
                            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                            if not texts:
                                logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©")
                                _embedding_progress["completed_texts"] += 1
                                _embedding_progress["completed_calls"] += 1
                                _log_embedding_progress()
                                return [[0.0] * _global_embedding_service.embedding_dim]
                            
                            # ë°°ì¹˜ ì„ë² ë”© ì‹œì‘ ë¡œê·¸
                            if len(texts) > 1:
                                logger.info(f"ğŸ”„ ì²­í¬ ì„ë² ë”© ì‹œì‘: {len(texts)}ê°œ ì²­í¬ ì²˜ë¦¬")
                            
                            # ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± (ìˆœì°¨ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± í–¥ìƒ)
                            results = []
                            batch_start_time = time.time()
                            
                            for i, text in enumerate(texts):
                                chunk_start_time = time.time()
                                
                                if not text or not str(text).strip():
                                    logger.warning(f"ë¹ˆ í…ìŠ¤íŠ¸ í•­ëª© {i+1}/{len(texts)}ì— ëŒ€í•œ ì„ë² ë”©")
                                    embedding = [0.0] * _global_embedding_service.embedding_dim
                                else:
                                    # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                                    max_retries = 2
                                    embedding = None
                                    for attempt in range(max_retries):
                                        try:
                                            embedding = await _global_embedding_service.embed(str(text))
                                            chunk_elapsed = time.time() - chunk_start_time
                                            
                                            # í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì²˜ë¦¬ ì‹œê°„ ì •ë³´
                                            text_len = len(str(text))
                                            if len(texts) > 5:  # ë§ì€ ì²­í¬ê°€ ìˆì„ ë•Œë§Œ ê°„í—ì  ë¡œê·¸
                                                if i % 10 == 0 or i == len(texts) - 1:
                                                    logger.info(f"   ğŸ“ ì²­í¬ {i+1}/{len(texts)} ì™„ë£Œ "
                                                              f"({text_len}ê¸€ì, {chunk_elapsed:.1f}ì´ˆ, ì°¨ì›:{len(embedding)})")
                                            else:
                                                logger.debug(f"í…ìŠ¤íŠ¸ {i+1}/{len(texts)} ì„ë² ë”© ì™„ë£Œ "
                                                           f"({text_len}ê¸€ì, {chunk_elapsed:.1f}ì´ˆ, ì°¨ì›:{len(embedding)})")
                                            break
                                        except Exception as e:
                                            logger.warning(f"ì„ë² ë”© ì„œë¹„ìŠ¤ í…ìŠ¤íŠ¸ {i+1} ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                                            if attempt < max_retries - 1:
                                                await asyncio.sleep(0.5)  # 0.5ì´ˆ ëŒ€ê¸°
                                            else:
                                                logger.error(f"í…ìŠ¤íŠ¸ {i+1} ì„ë² ë”© ì™„ì „ ì‹¤íŒ¨, ë”ë¯¸ ë²¡í„° ì‚¬ìš©")
                                                embedding = [0.0] * _global_embedding_service.embedding_dim
                                
                                results.append(embedding)
                                
                                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ê°œë³„ í…ìŠ¤íŠ¸ë³„)
                                _embedding_progress["completed_texts"] += 1
                                if len(texts) > 10 and (i % 5 == 0 or i == len(texts) - 1):  # 5ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ì— ë¡œê·¸
                                    _log_embedding_progress()
                                
                                # í…ìŠ¤íŠ¸ ê°„ ì•½ê°„ì˜ ì§€ì—°ìœ¼ë¡œ ì„œë²„ ë¶€í•˜ ì™„í™”
                                if i < len(texts) - 1:
                                    await asyncio.sleep(0.05)  # ì§€ì—° ì‹œê°„ ë‹¨ì¶•
                            
                            # ë°°ì¹˜ ì™„ë£Œ ë¡œê·¸
                            batch_elapsed = time.time() - batch_start_time
                            if len(texts) > 1:
                                avg_per_chunk = batch_elapsed / len(texts)
                                logger.info(f"âœ… ì²­í¬ ì„ë² ë”© ì™„ë£Œ: {len(texts)}ê°œ ì²­í¬, "
                                          f"ì´ {batch_elapsed:.1f}ì´ˆ (í‰ê·  {avg_per_chunk:.2f}ì´ˆ/ì²­í¬)")
                            
                            _embedding_progress["completed_calls"] += 1
                            return results  # ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜
                        else:
                            # ê¸°íƒ€ í˜•íƒœ ì²˜ë¦¬
                            text_str = str(texts)
                            if not text_str.strip():
                                _embedding_progress["completed_texts"] += 1
                                _embedding_progress["completed_calls"] += 1
                                _log_embedding_progress()
                                return [0.0] * _global_embedding_service.embedding_dim
                            
                            # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                            max_retries = 2
                            for attempt in range(max_retries):
                                try:
                                    result = await _global_embedding_service.embed(text_str)
                                    
                                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                                    _embedding_progress["completed_texts"] += 1
                                    _embedding_progress["completed_calls"] += 1
                                    _log_embedding_progress()
                                    
                                    return result
                                except Exception as e:
                                    logger.warning(f"embedding ì„œë¹„ìŠ¤ ì„ë² ë”© ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                                    if attempt < max_retries - 1:
                                        await asyncio.sleep(1)
                                    else:
                                        raise
                                
                    except Exception as e:
                        logger.error(f"ì„ë² ë”© í•¨ìˆ˜ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        # ë”ë¯¸ ë²¡í„° ë°˜í™˜
                        dummy_dim = _global_embedding_service.embedding_dim if _global_embedding_service else 1024
                        
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨í•œ ê²½ìš°ë„)
                        _embedding_progress["completed_texts"] += text_count
                        _embedding_progress["completed_calls"] += 1
                        _log_embedding_progress()
                        
                        if isinstance(texts, list) and len(texts) > 1:
                            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í•­ëª©ì— ëŒ€í•´ ë”ë¯¸ ë²¡í„° ìƒì„±
                            return [[0.0] * dummy_dim for _ in texts]
                        else:
                            # ë‹¨ì¼ ë²¡í„° ë°˜í™˜
                            return [0.0] * dummy_dim

            # LightRAGì—ì„œ EmbeddingFunc ë˜í¼ ì‚¬ìš© (embedding ì „ìš© ì„œë¹„ìŠ¤ì˜ ì°¨ì› ì‚¬ìš©)
            embedding_wrapper = EmbeddingFunc(
                embedding_dim=embedding_service.embedding_dim,
                max_token_size=8192,
                func=embedding_func
            )
            
            rag_instance = LightRAG(
                working_dir=str(settings.lightrag_working_dir),
                llm_model_func=llm_model_func,
                embedding_func=embedding_wrapper,
                chunk_token_size=settings.lightrag_chunk_size,
                chunk_overlap_token_size=settings.lightrag_chunk_overlap,
            )
            
            logger.debug("LightRAG ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹œì‘...")
            # LightRAG ì €ì¥ì†Œ ì´ˆê¸°í™” (í•„ìˆ˜)
            await rag_instance.initialize_storages()
            logger.debug("LightRAG ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            logger.debug("íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” ì‹œì‘...")
            # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” (í•„ìˆ˜)
            from lightrag.kg.shared_storage import initialize_pipeline_status
            await initialize_pipeline_status()
            logger.debug("íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
            
            logger.info(f"LightRAG ì´ˆê¸°í™” ì™„ë£Œ - ëŒ€í™”: {chat_provider}, Embedding: {embedding_provider}, KG: {kg_provider} (ì°¨ì›: {embedding_wrapper.embedding_dim})")
            logger.debug(f"ì‘ì—… ë””ë ‰í† ë¦¬: {settings.lightrag_working_dir}")
            logger.debug(f"ì²­í¬ í¬ê¸°: {settings.lightrag_chunk_size}, ì˜¤ë²„ë©: {settings.lightrag_chunk_overlap}")
            logger.info("ë™ì‹œì„± ì œì–´: LLM ì„¸ë§ˆí¬ì–´=2, ì„ë² ë”© ì„¸ë§ˆí¬ì–´=3")
            return rag_instance
        except Exception as e:
            logger.error(f"LightRAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def insert_documents(self, documents: Optional[List[Dict[str, str]]] = None, only_new: bool = True):
        """ë¬¸ì„œë¥¼ RAGì— ì‚½ì… (ì§„í–‰ë¥  ì¶”ì  í¬í•¨)
        
        Args:
            documents: ì‚½ì…í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ ë¡œë“œ)
            only_new: Trueë©´ ì‹ ê·œ/ë³€ê²½ëœ íŒŒì¼ë§Œ, Falseë©´ ëª¨ë“  íŒŒì¼
        """
        if self.rag is None:
            logger.error("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        try:
            if documents is None:
                documents = self.document_loader.load_documents(only_new=only_new)

            if not documents:
                if only_new:
                    logger.info("ìƒˆë¡œ ì¶”ê°€ë˜ê±°ë‚˜ ë³€ê²½ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    logger.warning("ì‚½ì…í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ì§„í–‰ë¥  ì¶”ì  ì´ˆê¸°í™”
            _reset_embedding_progress()
            global _embedding_progress
            _embedding_progress["start_time"] = time.time()
            
            # LightRAG ì „ìš© file_paths ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©
            contents = [doc['content'] for doc in documents]
            file_paths = [doc.get('relative_path', doc['name']) for doc in documents]
            
            # ë¬¸ì„œ ë‚´ìš© ë¡œê·¸ (ë””ë²„ê¹…ìš©)
            for i, doc in enumerate(documents):
                logger.debug(f"ë¬¸ì„œ {i+1} ({doc['path']}) ë‚´ìš© (ì²« 200ì): {doc['content'][:200]}")

            logger.info(f"ğŸ“š {len(documents)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì‹œì‘...")
            logger.info(f"ğŸ’¡ ê° ë¬¸ì„œë³„ ì§„í–‰ë¥ ê³¼ ì²˜ë¦¬ ì‹œê°„ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
            
            # ê° ë¬¸ì„œë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ê°œë³„ ì²˜ë¦¬
            success_count = 0
            for i, (content, file_path) in enumerate(zip(contents, file_paths)):
                try:
                    # ë¬¸ì„œë³„ ì‹œê°„ ì¸¡ì • ì‹œì‘
                    doc_start_time = time.time()
                    doc_name = f"{i+1}/{len(documents)} - {Path(file_path).name}"
                    
                    # ë¬¸ì„œ í¬ê¸° ì •ë³´
                    content_size = len(content)
                    estimated_chunks = max(1, content_size // settings.lightrag_chunk_size)
                    
                    logger.info(f"ğŸ“„ ë¬¸ì„œ ì„ë² ë”© ì‹œì‘: {doc_name}")
                    logger.info(f"   ğŸ“ í¬ê¸°: {content_size:,} ê¸€ì, ì˜ˆìƒ ì²­í¬: ~{estimated_chunks}ê°œ")
                    
                    # LightRAGì˜ file_paths ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¬ë°”ë¥¸ ì†ŒìŠ¤ ì •ë³´ ì œê³µ
                    try:
                        await self.rag.ainsert([content], file_paths=[file_path])
                        success_msg = f"âœ… ë¬¸ì„œ {i+1} ì„ë² ë”© ì„±ê³µ: {Path(file_path).name}"
                    except Exception as e:
                        logger.warning(f"file_paths ë°©ì‹ ì‹¤íŒ¨ (ë¬¸ì„œ {i+1}): {e}, ê¸°ë³¸ ë°©ì‹ ì‹œë„...")
                        # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ fallback
                        await self.rag.ainsert([content])
                        success_msg = f"âœ… ë¬¸ì„œ {i+1} ì„ë² ë”© ì„±ê³µ (ê¸°ë³¸ ë°©ì‹): {Path(file_path).name}"
                    
                    # ë¬¸ì„œë³„ ì‹œê°„ ì¸¡ì • ì™„ë£Œ
                    doc_elapsed = time.time() - doc_start_time
                    _embedding_progress["document_times"].append(doc_elapsed)
                    
                    # í‰ê·  ì‹œê°„ ê³„ì‚°
                    avg_time = sum(_embedding_progress["document_times"]) / len(_embedding_progress["document_times"])
                    remaining_docs = len(documents) - (i + 1)
                    eta = remaining_docs * avg_time if remaining_docs > 0 else 0
                    
                    logger.info(f"{success_msg}")
                    logger.info(f"   â±ï¸  ì†Œìš”ì‹œê°„: {doc_elapsed:.1f}ì´ˆ (í‰ê· : {avg_time:.1f}ì´ˆ)")
                    if remaining_docs > 0:
                        logger.info(f"   ğŸ• ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta:.0f}ì´ˆ ({remaining_docs}ê°œ ë¬¸ì„œ ë‚¨ìŒ)")
                    
                    success_count += 1
                    
                except Exception as e:
                    doc_elapsed = time.time() - doc_start_time
                    logger.error(f"âŒ ë¬¸ì„œ {i+1} ì„ë² ë”© ì‹¤íŒ¨ ({Path(file_path).name}): {e}")
                    logger.error(f"   â±ï¸  ì‹¤íŒ¨ê¹Œì§€ ì†Œìš”ì‹œê°„: {doc_elapsed:.1f}ì´ˆ")
                    continue
            
            # ì „ì²´ ì™„ë£Œ ë¡œê·¸
            total_time = time.time() - _embedding_progress["start_time"]
            avg_doc_time = sum(_embedding_progress["document_times"]) / len(_embedding_progress["document_times"]) if _embedding_progress["document_times"] else 0
            
            logger.info(f"ğŸ‰ ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ! ì„±ê³µ: {success_count}/{len(documents)}ê°œ")
            logger.info(f"ğŸ“Š ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ, ë¬¸ì„œë‹¹ í‰ê· : {avg_doc_time:.1f}ì´ˆ")
            logger.info(f"ğŸ“ˆ ì„ë² ë”© í†µê³„: ì´ {_embedding_progress['completed_texts']}ê°œ í…ìŠ¤íŠ¸ ì²­í¬, "
                       f"{_embedding_progress['completed_calls']}íšŒ API í˜¸ì¶œ")
            
            if _embedding_progress["completed_texts"] > 0:
                rate = _embedding_progress["completed_texts"] / total_time
                logger.info(f"âš¡ ì„ë² ë”© ì†ë„: {rate:.1f}ê°œ ì²­í¬/ì´ˆ")
            
            # ë¬¸ì„œë“¤ì„ ì„ë² ë”© ì™„ë£Œë¡œ í‘œì‹œ
            successful_docs = documents[:success_count]  # ì„±ê³µí•œ ë¬¸ì„œë“¤ë§Œ
            if successful_docs:
                self.document_loader.mark_documents_embedded(successful_docs)
            
            # ì¸ë±ì‹± ìƒíƒœ í™•ì¸
            try:
                storage_info = await self.get_indexed_info()
                logger.info(f"ğŸ’¾ ì¸ë±ìŠ¤ ìƒíƒœ: {storage_info}")
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨: {e}")
            raise

    async def query(self, question: str, mode: str = "hybrid") -> str:
        """RAGì— ì§ˆì˜"""
        # ë””ë²„ê¹…: ë©”ì„œë“œ ì§„ì… ì‹œì  ë¡œê·¸
        logger.debug(f"[QUERY_ENTRY] ë°›ì€ ì§ˆë¬¸: '{question}' (ê¸¸ì´: {len(question)}, íƒ€ì…: {type(question)})")
        logger.debug(f"[QUERY_ENTRY] ì§ˆë¬¸ ë°”ì´íŠ¸: {question.encode('utf-8')!r}")
        
        if self.rag is None:
            logger.error("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        try:
            logger.debug(f"ì§ˆì˜: {question} (ëª¨ë“œ: {mode})")
            # ë” ì•ˆì „í•œ ë¡œê·¸ ì¶œë ¥
            logger.debug(f"[SAFE_LOG] ì§ˆì˜: {repr(question)} (ëª¨ë“œ: {mode})")

            # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œì„ LLM ë‹¨ê³„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½
            original_question = question

            # --- ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (user) ---
            self.conversation_history.append({"role": "user", "content": original_question})

            # LightRAG API í˜¸í™˜ì„± ìˆ˜ì •
            from lightrag import QueryParam
            param = QueryParam(
                mode=mode,
                conversation_history=self.conversation_history,
                history_turns=settings.lightrag_history_turns,
            )
            
            logger.debug("LightRAG ì§ˆì˜ ì‹œì‘...")
            response = await self.rag.aquery(original_question, param=param)
            logger.debug(f"LightRAG ì‘ë‹µ íƒ€ì…: {type(response)}")
            logger.debug(f"LightRAG ì‘ë‹µ ë‚´ìš© (ì²« 200ì): {str(response)[:200]}")
            
            # ë¡œì»¬ APIì— ì§ˆë¬¸ê³¼ ë‹µë³€ ì „ì†¡
            try:
                await self.local_api.send_rag_response(original_question, response)
            except Exception as api_error:
                logger.warning(f"ë¡œì»¬ API ì „ì†¡ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {api_error}")
            
            # ì‘ë‹µì´ dict í˜•íƒœì¸ ê²½ìš° ì²˜ë¦¬
            if isinstance(response, dict):
                if 'response' in response:
                    answer = response['response']
                elif 'answer' in response:
                    answer = response['answer']
                else:
                    # dictì˜ ì²« ë²ˆì§¸ ê°’ì„ ë°˜í™˜í•˜ê±°ë‚˜ ë¬¸ìì—´ë¡œ ë³€í™˜
                    answer = str(response)
            else:
                answer = response
            
            logger.debug(f"ì²˜ë¦¬ëœ ë‹µë³€: {answer}")
            
            # no-context ì‘ë‹µì¸ ê²½ìš° ë‹¤ë¥¸ ëª¨ë“œë¡œ ì¬ì‹œë„
            if answer and "[no-context]" in str(answer):
                logger.warning(f"{mode} ëª¨ë“œì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. naive ëª¨ë“œë¡œ ì¬ì‹œë„...")
                try:
                    param = QueryParam(
                        mode="naive",
                        conversation_history=self.conversation_history,
                        history_turns=settings.lightrag_history_turns,
                    )
                    response = await self.rag.aquery(original_question, param=param)
                    if isinstance(response, dict):
                        if 'response' in response:
                            answer = response['response']
                        elif 'answer' in response:
                            answer = response['answer']
                        else:
                            answer = str(response)
                    else:
                        answer = response
                    logger.debug(f"naive ëª¨ë“œ ì‘ë‹µ: {answer}")
                except Exception as e:
                    logger.warning(f"naive ëª¨ë“œ ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                
            # í•œêµ­ì–´ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” LLMì—ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë„ë¡ í›„ì²˜ë¦¬
            if answer and self._is_korean(original_question) and not self._is_korean(answer) and "[no-context]" not in str(answer):
                # í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ë‹µë³€ ìš”ì²­
                korean_prompt = f"ë‹¤ìŒ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”:\n\n{answer}"
                try:
                    if _global_llm_service is not None:
                        async with _llm_semaphore:  # ì„¸ë§ˆí¬ì–´ ì‚¬ìš©
                            korean_answer = await _global_llm_service.generate(korean_prompt)
                            # assistant ë©”ì‹œì§€ ì €ì¥
                            final_ans = korean_answer if korean_answer else answer
                            self.conversation_history.append({"role": "assistant", "content": str(final_ans)})
                            return final_ans
                    else:
                        logger.warning("ì „ì—­ LLM ì„œë¹„ìŠ¤ê°€ ì—†ì–´ í•œêµ­ì–´ ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤")
                        self.conversation_history.append({"role": "assistant", "content": str(answer)})
                        return answer
                except Exception as e:
                    logger.warning(f"í•œêµ­ì–´ ë²ˆì—­ ì‹¤íŒ¨: {e}")
                    self.conversation_history.append({"role": "assistant", "content": str(answer)})
                    return answer
            
            if answer:
                self.conversation_history.append({"role": "assistant", "content": str(answer)})
                return answer
            else:
                return "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            logger.error(f"ì§ˆì˜ ì‹¤íŒ¨: {e}")
            logger.exception("ì§ˆì˜ ì‹¤íŒ¨ ìƒì„¸ ì •ë³´")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _is_korean(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ í•œêµ­ì–´ì¸ì§€ í™•ì¸"""
        korean_chars = sum(1 for char in text if 'ê°€' <= char <= 'í£')
        return korean_chars / len(text) > 0.3 if text else False

    async def get_indexed_info(self) -> Dict[str, Any]:
        """ì¸ë±ì‹±ëœ ì •ë³´ ë°˜í™˜"""
        try:
            storage_path = Path(settings.lightrag_working_dir)
            info: Dict[str, Any] = {
                'working_dir': str(storage_path),
                'exists': storage_path.exists(),
                'files': [],
                'llm_service': settings.get_llm_service(),
                'embedding_llm_service': settings.get_embedding_llm_service(),
                'kg_llm_service': settings.get_kg_llm_service(),
                'chunk_size': settings.lightrag_chunk_size,
                'chunk_overlap': settings.lightrag_chunk_overlap,
                'embedding_model': settings.lightrag_embedding_model,
                'llm_provider': settings.llm_provider,
                'embedding_llm_provider': settings.embedding_llm_provider,
                'kg_llm_provider': settings.kg_llm_provider,
                'local_api_host': settings.local_api_host,
            }

            if storage_path.exists():
                for file in storage_path.iterdir():
                    if file.is_file():
                        info['files'].append({
                            'name': file.name,
                            'size': file.stat().st_size
                        })

            # ë¡œì»¬ API ìƒíƒœ í™•ì¸
            info["local_api_available"] = await self.local_api.is_api_available()

            return info
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)} 